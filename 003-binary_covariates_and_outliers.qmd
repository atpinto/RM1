```{r, setup, include=FALSE}
library(ggplot2)
library(Statamarkdown)
stataexe <- "/Applications/Stata/StataBE.app/Contents/MacOS/StataBE"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
#knitr::opts_knit$set(root.dir = '../Data') # Changes the working director to the Data folder
```

# Binary Covariates, Outliers, and Influential Observations {#binary_outliers}

## Learning objectives {#learn_obj_wk03 .unnumbered}

By the end of this week you should be able to:

1.  Formulate, carry out and interpret a simple linear regression with a binary independent variable

2.  Check for outliers and influential observations using measures and diagnostic plots

3.  Understand the key principals of methods to deal with outliers and influential observations

## Learning activities {#learn_act_wk03 .unnumbered}

This week's learning activities include:

| Learning Activity | Learning objectives |
|-------------------|---------------------|
| Video 1           | 1                   |
| Notes&Readings    | 2, 3                |

## Binary covariates {.unnumbered}

::::: grid
::: g-col-9
So far we have limited our analysis to continuous independent variables (e.g. age). This week we are going to explore the situation where the independent variable (the $x$ variable) is binary.
:::
::: g-col-3
<a href="https://www.youtube.com/embed/BENjkcPL5uo" onclick="window.open(this.href, 'videoPopup', 'width=800,height=600'); return false;"> <img src="https://img.youtube.com/vi/BENjkcPL5uo/hqdefault.jpg" width="200"/> </a>
:::
:::::


## Outliers and influential observations {.unnumbered}

An outlier is a point with a large residual. Sometimes an outlier can have a large impact on the estimates of the regression parameter. If you move some of the points in the scatter so they become outliers (far from other points), you can see this will affect the estimated regression line. However, not all outliers are the same. Try moving up and down one of the points at the beginning or the end of the X scale. The impact in the regression line is much stronger than if you do the same with a point in the mid range of X.

```{=html}
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Regression Plot</title>
    <style>
        canvas {
            border: 2px solid #333;
            background: #f9f9f9;
            display: block;
            margin: 20px auto;
            cursor: crosshair;
        }
    </style>
</head>
<body>
    <h4><center>Interactive Regression Plot</center></h4>
    <canvas id="canvas" width="640" height="400"></canvas>
    <script>
        window.onload = function() {
            var n = 29;
            var pointSize = 6;
            var drag_point = -1;
            var canvas = document.getElementById("canvas");
            var ctx = canvas.getContext("2d");

            var points = [
                {x: 39*9, y: 144*2}, {x: 45*9, y: 138*2}, {x: 47*9, y: 145*2}, 
                {x: 65*9, y: 162*2}, {x: 46*9, y: 142*2}, {x: 67*9, y: 170*2}, 
                {x: 42*9, y: 124*2}, {x: 67*9, y: 158*2}, {x: 56*9, y: 154*2}, 
                {x: 64*9, y: 162*2}, {x: 56*9, y: 150*2}, {x: 59*9, y: 140*2}, 
                {x: 34*9, y: 110*2}, {x: 42*9, y: 128*2}, {x: 48*9, y: 130*2}, 
                {x: 45*9, y: 135*2}, {x: 17*9, y: 114*2}, {x: 20*9, y: 116*2}, 
                {x: 19*9, y: 124*2}, {x: 36*9, y: 136*2}, {x: 50*9, y: 142*2}, 
                {x: 39*9, y: 120*2}, {x: 21*9, y: 120*2}, {x: 44*9, y: 160*2}, 
                {x: 53*9, y: 158*2}, {x: 63*9, y: 144*2}, {x: 29*9, y: 130*2}, 
                {x: 25*9, y: 125*2}, {x: 69*9, y: 175*2}
            ];

            canvas.onmousedown = function(e) {
                var pos = getPosition(e);
                drag_point = getPointAt(pos.x, pos.y);
                if (drag_point == -1) {
                    points.push(pos);
                    n++;
                    redraw();
                }
            };

            canvas.onmousemove = function(e) {
                if (drag_point !== -1) {
                    var pos = getPosition(e);
                    points[drag_point].x = pos.x;
                    points[drag_point].y = pos.y;
                    redraw();
                }
            };

            canvas.onmouseup = function() {
                drag_point = -1;
            };

            function getPosition(event) {
                var rect = canvas.getBoundingClientRect();
                return {x: event.clientX - rect.left, y: event.clientY - rect.top};
            }

            function getPointAt(x, y) {
                for (var i = 0; i < points.length; i++) {
                    if (Math.abs(points[i].x - x) < pointSize && Math.abs(points[i].y - y) < pointSize)
                        return i;
                }
                return -1;
            }

            function redraw() {
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                drawRegressionLine();
                drawPoints();
            }

            function drawRegressionLine() {
                var sum_x = 0, sum_y = 0, sum_xy = 0, sum_xx = 0, sum_yy = 0;

                for (var i = 0; i < n; i++) {
                    sum_x += points[i].x;
                    sum_y += points[i].y;
                    sum_xy += points[i].x * points[i].y;
                    sum_xx += points[i].x * points[i].x;
                    sum_yy += points[i].y * points[i].y;
                }

                var slope = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x * sum_x);
                var intercept = (sum_y - slope * sum_x) / n;

                ctx.lineWidth = 3;
                ctx.strokeStyle = 'red';
                ctx.beginPath();
                ctx.moveTo(0, intercept);
                ctx.lineTo(canvas.width, intercept + slope * canvas.width);
                ctx.stroke();

                ctx.fillStyle = "black";
                ctx.font = "bold 18px Arial";
                ctx.fillText(`y = ${intercept.toFixed(2)} + ${slope.toFixed(2)}x`, 20, 30);
            }

            function drawPoints() {
                points.forEach((p, index) => {
                    ctx.beginPath();
                    ctx.arc(p.x, p.y, pointSize, 0, Math.PI * 2, true);
                    ctx.fillStyle = (drag_point === index) ? "orange" : "blue";
                    ctx.fill();
                    ctx.strokeStyle = "black";
                    ctx.stroke();
                });
            }

            redraw();
        };
    </script>
</body>
```
::::: grid
::: g-col-7
Conversely, a data point may not have a large residual but still have an important influence in the estimated regression line. Below, you can see that the data point in the left does not appear to have a large residual but it strongly affects the regression line. In the figure on the right, we can see that despite the main cloud of points not suggesting any trend, the extreme left point has a high influence in the final regression line.
:::
::: g-col-5
![](images/image-1341453037.png){fig-align="center"}
:::
:::::

There are several statistics to measure and explore outliers and influential points. We will discuss some here:

::::: grid
::: g-col-9
**Leverage**: measures of how far away each value of the independent variable is from the others. Data with high-leverage points, if any, are outliers with respect to the independent variables. The levarage ($L$) varies between 0 and 1.
:::
::: {.g-col-3 .border .p-3}
**Formula** 
$L = \frac{1}{n} + \frac{(x_i-\bar{x})^2}{\sum{(x_i - \bar{x})^2}}$ 
:::
:::::


Consider the simple example below with 10 simulated data points. The 10th value for X was chosen to be far from the others.

**R code**

```{r, collapse = TRUE, include=TRUE}
set.seed(1011)
x<-rnorm(9)               #random 9 values
x[10]<-5                  #value far from the others
y<-rnorm(10,0.5+2*x,1)   #generate y

#plot the data
lmodel<-lm(y~x)           #fit the model
plot(x,y)                 #plot the data
abline(line(x,y))        # add the regression line
```

**Stata code**

```{stata, collectcode=TRUE, collapse=TRUE }
clear
set seed 1011
set obs 10

generate x = rnormal()
replace x = 5 in 10
generate y = rnormal(0.5+2*x,1)

graph twoway (lfit y x) (scatter y x)
regress y x      
```

If we compute the leverage of each point, it is not surprising that x=5 has a high leverage.

**R code**

```{r, collapse = TRUE, include=TRUE}
influence(lmodel)$hat     #leverage
1/10 + (x-mean(x))^2/(var(x)*9)  #leverage manually computed 
```

**Stata code**

```{stata, collectcode=TRUE, collapse=TRUE }
	clear
	set seed 1011

	*----------------------------
	* Generate the data
	*----------------------------
	set obs 10
	generate id = _n             // create an id for later merging
	generate x = rnormal()
	replace x = 5 in 10
	* Note: In Stata the typical way to generate a normal variable with mean mu and sd sigma is:
	*       generate y = (0.5+2*x) + sigma*rnormal()
	* Here we mimic the provided code.
	generate y = (0.5+2*x) + rnormal()

	*----------------------------
	* Estimate the full model and save coefficients
	*----------------------------
	regress y x
 
	*----------------------------
  *Leverage
	*----------------------------
  predict lev, leverage

	*----------------------------
  *Leverage computed manually
	*----------------------------	
  gen lev_manual = 1/10 + (x- .9228745)^2/( 1.563043^2 *9)

  list lev, lev_manual
```

**DFBETA**: We can also compute how the coefficients change if each observation is removed from the data. This will produce a vector for $\beta_0$ and $\beta_1$ corresponding to $n$ regressions fitted by deleting each observation at a time. The difference between the full data estimates and the estimates by removing each data point is called *DFBETA*. In the example of the small simulated dataset set above, the dfbeta can also be obtained from the `influence()` function in r.

**R code**

```{r, ccollapse = TRUE, include=TRUE}
influence(lmodel)$coefficients     #DFBETA
#dfbeta(lmodel)                    #does the same thing

#computing the DFBETA manually for the 10th observation
coef(lm(y~x)) - coef(lm(y[-10]~x[-10]))
```

**Stata code**

```{stata, collectcode=TRUE, collapse=TRUE }
*I am not aware of a function in Stata to compute 
*the unstandardised version of DFBETA.
*This script computes de unstandardise DFBETA "manually"

	clear
	set seed 1011

	*----------------------------
	* Generate the data
	*----------------------------
	set obs 10
	generate id = _n             // create an id for later merging
	generate x = rnormal()
	replace x = 5 in 10
	* Note: In Stata the typical way to generate a normal variable with mean mu and sd sigma is:
	*       generate y = (0.5+2*x) + sigma*rnormal()
	* Here we mimic the provided code.
	generate y = (0.5+2*x) + rnormal()

	*----------------------------
	* Estimate the full model and save coefficients
	*----------------------------
	regress y x
	scalar b0_full = _b[_cons]
	scalar b1_full = _b[x]

	*----------------------------
	* Loop over each observation to compute DFBETA
	*----------------------------
	* We will use a postfile to collect the results.
	tempname results
	tempfile dfbeta

	postfile `results' id dfb0 dfb1 using "`dfbeta'", replace

	forvalues i = 1/`=_N' {
		preserve
		drop if id == `i'
		quietly regress y x
		scalar dfb0 = _b[_cons] - b0_full   // change in intercept
		scalar dfb1 = _b[x] - b1_full        // change in slope for x
		post `results' (`i') (dfb0) (dfb1)
		restore
	}

	postclose `results'

	*----------------------------
	* Merge the DFBETA results back into the original dataset and list
	*----------------------------
	merge 1:1 id using "`dfbeta'", nogen
	sort id
	
	
	list dfb0 dfb1, sep(0)

```

Note that the *DFBETA* above are computed in the original scale of the data. Thus, the magnitude of the difference is dependent on this scale. An alternative is to standardise the *DFBETA* using the standard errors. This will give as deviance from the original estimate in standard errors. A common cut-off for a very strong influence in the results is the value $2$.

```{r, ccollapse = TRUE, include=TRUE}
 #computes the standardised dfbeta. 
#note that there is also a 
#dfbeta() function that computes the non-stantandardised dfbeta
dfbetas(lmodel)                   
```

**Stata code**

```{stata, collectcode=TRUE, collapse=TRUE }
  clear
  set seed 1011

	*----------------------------
	* Generate the data
	*----------------------------
	set obs 10
	generate id = _n             // create an id for later merging
	generate x = rnormal()
	replace x = 5 in 10
	* Note: In Stata the typical way to generate a normal variable with mean mu and sd sigma is:
	*       generate y = (0.5+2*x) + sigma*rnormal()
	* Here we mimic the provided code.
	generate y = (0.5+2*x) + rnormal()

 	*----------------------------
	* Estimate the full model 
	*---------------------------- 
  regress y x

	*----------------------------
	* Standardised DFBETA 
	*---------------------------- 
  dfbeta
  list 

```

In the example above, the 10th observation seems to haves reasonable impact in the estimates.


::::: grid
::: g-col-9
**Cook's distance** - This is another measure of influence that combines the leverage of the data point and its residual. It summarizes how much all the values in the regression model change when each observation is removed. 
:::
::: {.g-col-3 .border .p-3}
**Formula**
$D_j = \frac{\sum(\hat Y_i-\hat Y_{(-j)})^2}{2\sigma^2}$
:::
:::::

A general rule of thumb is that a point with a Cook's Distance ($D_j$) above $4/n$ is considered to be an outlier.

**R code**

```{r, ccollapse = TRUE, include=TRUE}
#the column cook.d is the Cook's distance
#note that this function also computes some of the measures discussed above
influence.measures(lmodel)                   
```

**Stata code**

```{stata, collectcode=TRUE, collapse=TRUE }
  clear
  set seed 1011

	*----------------------------
	* Generate the data
	*----------------------------
	set obs 10
	generate id = _n             // create an id for later merging
	generate x = rnormal()
	replace x = 5 in 10
	* Note: In Stata the typical way to generate a normal variable with mean mu and sd sigma is:
	*       generate y = (0.5+2*x) + sigma*rnormal()
	* Here we mimic the provided code.
	generate y = (0.5+2*x) + rnormal()

 	*----------------------------
	* Estimate the full model 
	*---------------------------- 
  regress y x

	*----------------------------
	* Cook distance
	*---------------------------- 
  predict cook_d, cook
  list 

```

With the example using the simulated sample with 10 observations, the rule of thumb would be $4/10$. Again, the 10th observation is above this threshold and would requires some consideration.

**Plots:** The above mesaures are commonly represented in a graphical way. There are many variations of these plots. Below are some examples of these plots but many other plots are available in different packages.

**R code**

```{r, ccollapse = TRUE, include=TRUE}
#leverage vs residuals
#A data point with high leverage and high residual may be problematic
plot(lmodel,5)                   

#Cook's distance
plot(lmodel,4)                   

#Leverage vs Cook's distance 
plot(lmodel,6)                   
```

**Stata code**

```{stata, collectcode=TRUE, collapse=TRUE }
  clear
  set seed 1011

	*----------------------------
	* Generate the data
	*----------------------------
	set obs 10
	generate id = _n             // create an id for later merging
	generate x = rnormal()
	replace x = 5 in 10
	* Note: In Stata the typical way to generate a normal variable with mean mu and sd sigma is:
	*       generate y = (0.5+2*x) + sigma*rnormal()
	* Here we mimic the provided code.
	generate y = (0.5+2*x) + rnormal()

 	*----------------------------
	* Estimate the full model 
	*---------------------------- 
  regress y x

 	*----------------------------
	* Leverage's plot
	*---------------------------- 
  lvr2plot

```

![](images/image-1783609938.png){width="400"}

### Book Chapter 4. Outlying, High Leverage, and Influential Poitns 4.7.4 (pages 124-128).  {.unnumbered}

This reading supplements the notes above with emphasis in the DFBETA plots. Note that this subchapter appears in the book after the extension of simple linear regression to the use of multiple independent variables (covariates) in the regression model, which we did not yet cover. However, there are only a few references to the multiple linear regression case.

## Exercises: {.unnumbered}

The dataset [lowbwt.csv](https://www.dropbox.com/s/r06u1l1cjrvcpl1/lowbwt.csv?dl=1) was part of a study aiming to identify risk factors associated with giving birth to a low birth weight baby (weighing less than 2500 grams).

1 - Fit a linear model for the variable *bwt* (birth weight) using the covariate *age* (mother's age), evaluate the assumptions and interpret the results.

```{r, include=F, eval=F}
lowbwt <- read.csv("https://www.dropbox.com/s/r06u1l1cjrvcpl1/lowbwt.csv?dl=1")   
lm1 <- lm(bwt~age, data=lowbwt)
```

2 - Evaluate potential outliers and influential observations. How would the results change if you excluded this/these observation(s)?

```{r, include=F, eval=F}
plot(lm1, 4)
plot(lm1, 5)
plot(lm1, 6)
inf <- influence.measures(lm1)
boxplot(inf$infmat[,1],inf$infmat[,2])

```

## Summary {.unnumbered}

This weeks key concepts are:

::: {#box1 .green-box}
- The key concepts around binary variables will be added here after you have had a chance to finish your independent investigation.

- Outliers are observations with a very large absolute residual value. That is, we normally refer to outliers as observations with extreme values in the outcome variable $Y$. Outliers in the covariate $x$ are observations with high *leverage*. The precise formula for leverage is less important than understanding how high leverage observations can impact your regression.

- A residual versus leverage plot is a very useful diagnostic to see which observations may be highly influential
  - Observations that are outliers (in the outcome) and that have low leverage, may influence the intercept of your regression model
  - Observations that are not outliers, but have high leverage might artificially inflate the precision of your regression model
  - Observations that are outliers AND have high leverage may influence the intercept and slope of your regression model

- When potentially highly influential observations are detected, a sensitivity analysis where the results are compared with and without those observations is a useful tool for measuring influence.
:::
